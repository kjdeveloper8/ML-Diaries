{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc9d8705-c920-4ab7-8bb8-0e21ebc758d7",
   "metadata": {},
   "source": [
    "## CNN \n",
    "\n",
    "Convolution Neural Network is a type of neural network that is well suited to detecting spatial substructure by having a small number of weights they use to scan the input data tensors. From this scanning, they produce output tensors that represent the detection (or not) of substructures.\n",
    "\n",
    "- Conv1d: useful for time series in which each time step has a feature vector(can learn patterns on the sequence dimension) - nlp\n",
    "- Conv2d: capture spatio temporal patterns along two directions in the data, for example, in images along the height and width dimensions - image processing.\n",
    "- Conv3d: captures convolutions the patterns along three dimensions in the data - video\n",
    "\n",
    "### Hyperparams\n",
    "\n",
    "#### - Kernel\n",
    "  A kernel is a small square matrix that is applied at different positions in the input matrix in a systematic way and width of kernel is called kernel_size\n",
    "  \n",
    "#### - Channel\n",
    "  The feature dimension along each point in the input\n",
    "\n",
    "#### - Stride\n",
    "  Stride controls the step size between convolutions. If the stride is the same size as the kernel, the kernel\n",
    "  computations do not overlap. On the other hand, if the stride is 1, the kernels are maximally overlapping. The output\n",
    "  tensor can be deliberately shrunk to summarize information by increasing the stride.\n",
    "\n",
    "#### - Padding\n",
    "  To avoid shrinking feature size, padding is used by appending and prepending 0s to each respective dimension\n",
    "\n",
    "#### - Dialation\n",
    "  Dilation controls how the convolutional kernel is applied to the input matrix. If increasing the dilation from 1 (the\n",
    "  default) to 2 means that the elements of the kernel are two spaces away from each other when applied to the input\n",
    "  matrix.\n",
    "\n",
    "#### - Pooling\n",
    "  Pooling is an operation to summarize a higher-dimensional feature map to a lower-dimensional feature map\n",
    "\n",
    "#### - Batch Normalization\n",
    "  BatchNorm applies a transformation to the output of a CNN by scaling the activations to have zero mean and unit variance\n",
    "\n",
    "#### - Residual connection / skip connection\n",
    "  For the input to be added to the output of the convolution, they must have the same shape. And for this padding is\n",
    "  used. Residual conn is method to apply this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee012a7-153e-4cb5-9735-f4c4e657bed3",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef33e4d5-3d71-4d10-831b-37d1c28b15c9",
   "metadata": {},
   "source": [
    "#### conv1D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a94ddb5f-89c0-44b8-b7f1-b7689808ea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b0f5deb-ff43-477e-99da-5625860f85c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original data size --> torch.Size([2, 10, 7])\n",
      "after(conv1D) intermediate data size --> torch.Size([2, 16, 5])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "one_hot_size = 10\n",
    "sequence_width = 7\n",
    "\n",
    "data = torch.randn(batch_size, one_hot_size, sequence_width) # 3d data\n",
    "conv1 = nn.Conv1d(in_channels=one_hot_size, out_channels=16, kernel_size=3) \n",
    "intermediate1 = conv1(data)\n",
    "\n",
    "print(f'original data size --> {data.size()}') \n",
    "print(f'after(conv1D) intermediate data size --> {intermediate1.size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b628f09-d6e1-4e7f-ba60-75765031ffb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(conv2d) intermediate data size --> torch.Size([2, 32, 3])\n"
     ]
    }
   ],
   "source": [
    "# conv2d\n",
    "conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3) \n",
    "intermediate2 = conv2(intermediate1) \n",
    "print(f'(conv2d) intermediate data size --> {intermediate2.size()}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9abaf786-1e65-441a-ab5f-6950d3067f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(conv3d) intermediate data size --> torch.Size([2, 64, 1])\n"
     ]
    }
   ],
   "source": [
    "# conv3d\n",
    "conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "intermediate3 = conv3(intermediate2)\n",
    "print(f'(conv3d) intermediate data size --> {intermediate3.size()}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cd4dc86-61d5-4c14-9ebb-85bc64a439ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size --> torch.Size([2, 64])\n"
     ]
    }
   ],
   "source": [
    "# output\n",
    "y_output = intermediate3.squeeze() \n",
    "print(f'output size --> {y_output.size()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999589bd-8b8b-45a1-b3ad-3b343e3df8b9",
   "metadata": {},
   "source": [
    "#### reducing to feature vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21ac2af0-2ece-42e7-af43-b451b93481a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 80])\n"
     ]
    }
   ],
   "source": [
    "print(intermediate1.view(batch_size, -1).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6034e6a3-4148-49dc-ba45-7293176fc71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16])\n"
     ]
    }
   ],
   "source": [
    "print(torch.mean(intermediate1, dim=2).size()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bd1fadf-f7dd-4fec-afaf-04ee8fc970d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.max(\n",
      "values=tensor([[0.8763, 1.4122, 0.8265, 0.7341, 0.6411, 0.6215, 0.9622, 1.1170, 0.1178,\n",
      "         0.0103, 0.6448, 0.4109, 1.4061, 0.6290, 0.9599, 0.3937],\n",
      "        [0.7660, 0.9959, 0.6734, 1.0056, 0.5020, 0.9943, 0.5043, 0.4783, 0.3410,\n",
      "         0.5387, 0.7351, 0.3574, 0.7225, 0.3733, 0.5226, 0.3930]],\n",
      "       grad_fn=<MaxBackward0>),\n",
      "indices=tensor([[3, 4, 4, 2, 1, 3, 3, 3, 4, 4, 0, 0, 3, 4, 2, 3],\n",
      "        [0, 1, 3, 4, 0, 0, 1, 2, 4, 4, 0, 2, 4, 3, 0, 2]]))\n"
     ]
    }
   ],
   "source": [
    "print(torch.max(intermediate1, dim=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49e59b1c-8613-4ea8-90cc-460f6ed6b39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16])\n"
     ]
    }
   ],
   "source": [
    "print(torch.sum(intermediate1, dim=2).size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
