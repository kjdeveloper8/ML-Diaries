{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45b35de5-6a38-420a-9572-b4fa0412c780",
   "metadata": {},
   "source": [
    "## Pytorch basics: Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a30afd-0ffe-4625-b4c3-88bd6eb1f6d7",
   "metadata": {},
   "source": [
    "#### Gradient Descent\n",
    "\n",
    "One basic first-order optimisation algorithm for minimising loss function $ L $ is called gradient descent.\n",
    "\n",
    "$$\n",
    "G = \\theta \\longleftarrow \\theta - \\lambda \\frac{\\partial L}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "$ \\theta $ : inputs of the loss function \\\n",
    "$ \\lambda $ : learning rate \\\n",
    "$ \\frac{\\partial L}{\\partial \\theta} $ : gradient of L with respect to its parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53983bab-f966-45e1-a6e5-c69bb61ff3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_function(theta):\n",
    "    # square loss function\n",
    "    return 2 * theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2f42fbb-ca61-467b-af68-13c4ad6d7cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 step gredient descent\n",
    "def one_step_gradient_descent(theta, lr):\n",
    "    \"\"\" Returns updated value of theta after one step. \"\"\"\n",
    "    return theta - lr * square_function(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b76b410-0e70-47a2-a33b-8ba4d48e51d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_step_gradient_descent(1, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1a78e0d-d968-429d-be8b-747133452da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğœƒ:  0.6\n",
      "ğœƒ:  0.36\n",
      "ğœƒ:  0.216\n",
      "ğœƒ:  0.1296\n",
      "ğœƒ:  0.07776\n",
      "ğœƒ:  0.046655999999999996\n",
      "ğœƒ:  0.027993599999999997\n",
      "ğœƒ:  0.016796159999999997\n",
      "ğœƒ:  0.010077695999999997\n",
      "ğœƒ:  0.006046617599999998\n",
      "ğœƒ:  0.003627970559999999\n",
      "ğœƒ:  0.002176782335999999\n",
      "ğœƒ:  0.0013060694015999993\n",
      "ğœƒ:  0.0007836416409599996\n",
      "ğœƒ:  0.00047018498457599973\n",
      "ğœƒ:  0.00028211099074559984\n",
      "ğœƒ:  0.00016926659444735988\n",
      "ğœƒ:  0.00010155995666841592\n",
      "ğœƒ:  6.093597400104955e-05\n",
      "ğœƒ:  3.656158440062973e-05\n",
      "ğœƒ after optimization: 3.656158440062973e-05\n"
     ]
    }
   ],
   "source": [
    "# multiple 1 step gredient descent\n",
    "def gradient_descent(initial_theta, lr, steps):\n",
    "    \"\"\" Returns final value of theta after multiple one step gradient descents. \"\"\"\n",
    "    theta = initial_theta # float\n",
    "    for _ in range(steps):\n",
    "        theta = one_step_gradient_descent(theta, lr)\n",
    "        print(\"ğœƒ: \",theta)\n",
    "    return theta\n",
    "\n",
    "theta_opt = gradient_descent(\n",
    "    initial_theta=1.0,\n",
    "    lr=0.2,\n",
    "    steps=20\n",
    ")\n",
    "print(f\"ğœƒ after optimization: {theta_opt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e938a9b4-bfb4-49c4-a688-a5062e257c07",
   "metadata": {},
   "source": [
    "### Automatic Gradient Descent in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d7d7097-1ec4-47e6-9e54-4e0bcdb9c15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5595aeb6-3cfa-40df-bc9d-0a8ea80a71e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.Tensor([[1, 2], [3, 4]])\n",
    "parameters = torch.nn.Parameter(tensor) # stores operations applied to\n",
    "print(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9190b4f6-80c5-441d-91f0-f3572a5b7398",
   "metadata": {},
   "source": [
    "requires_grad=True indicates that the parameters will keep track of all the operations applied, so that the gradients can be computed automatically when needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e291367-b836-4ba2-9096-c02adca32787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6., 7.],\n",
      "        [8., 9.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# everytime we perform an operation on a torch Parameter, \n",
    "# it is recorded in each computed tensor.\n",
    "temp = parameters + 5\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dec7bb-6b66-47b5-b31a-12d68d6f922b",
   "metadata": {},
   "source": [
    "grad_fn=<AddBackward0> indicates that it remembers the last operation performed on the parameters (addition with 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bd14b1f-5c6a-4863-9588-513509f72c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.7183,  7.3891],\n",
      "        [20.0855, 54.5982]], grad_fn=<ExpBackward0>)\n"
     ]
    }
   ],
   "source": [
    "temp = torch.exp(parameters)\n",
    "print(temp)\n",
    "# remembered operation performed exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fdb1c40a-fc02-406e-8786-5a4b2e6aaffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "t0 = torch.Tensor([1])\n",
    "theta = torch.nn.Parameter(t0)\n",
    "\n",
    "loss = theta * theta\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6067616a-fc2f-48d3-9be9-9ef876200076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(theta.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224f9f6e-8b96-4ff7-a252-2bc67de47020",
   "metadata": {},
   "source": [
    "here it is None as we did not tell torch which gradient we wanted to calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b8a5d18-b513-4c18-b95b-b38cb0e3a60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "loss = theta * theta\n",
    "loss.backward()\n",
    "print(theta.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafb4f00-f3d1-4531-8493-800daee9dc24",
   "metadata": {},
   "source": [
    "#### Calculate automatic gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a68b8e7-d061-4df6-8b0f-0f97f176ee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_autograd(tensor):\n",
    "    tensor_with_grad = torch.nn.Parameter(tensor) \n",
    "    loss = tensor_with_grad * tensor_with_grad  # loss = theta * theta\n",
    "    loss.backward() \n",
    "    return tensor_with_grad.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "80dff588-7c19-4595-a857-ff9d39e19bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd_torch_autograd(initial_theta, lr, steps):\n",
    "    \"\"\" Returns automatic gradient. \"\"\"\n",
    "    tensor = initial_theta\n",
    "    for _ in range(steps):\n",
    "        tensor = tensor - lr * gradient_autograd(tensor)\n",
    "        print(tensor, \"-- gd: \", gradient_autograd(tensor))\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ca0a5e0-284b-4bc4-8593-b7a843012d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6000]) -- gd:  tensor([1.2000])\n",
      "tensor([0.3600]) -- gd:  tensor([0.7200])\n",
      "tensor([0.2160]) -- gd:  tensor([0.4320])\n",
      "tensor([0.1296]) -- gd:  tensor([0.2592])\n",
      "tensor([0.0778]) -- gd:  tensor([0.1555])\n",
      "tensor([0.0467]) -- gd:  tensor([0.0933])\n",
      "tensor([0.0280]) -- gd:  tensor([0.0560])\n",
      "tensor([0.0168]) -- gd:  tensor([0.0336])\n",
      "tensor([0.0101]) -- gd:  tensor([0.0202])\n",
      "tensor([0.0060]) -- gd:  tensor([0.0121])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0060])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_theta = torch.Tensor([1])\n",
    "gd_torch_autograd(initial_theta, lr=0.2, steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3b9d3a1-efed-487f-918c-d5c1bf831199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_autograd(initial_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faf843b-85d7-48b4-84bb-495074136bad",
   "metadata": {},
   "source": [
    "##### let's calculate\n",
    "\n",
    "tensor = tensor - lr * gradient_autograd(tensor)\n",
    "\n",
    "1 - 0.2 (2) = 0.6              -- g: 2*0.6 = 1.200 \\\n",
    "0.6000 - 0.2 (1.200) = 0.3600  -- g: 2*0.3600 = 0.7200 \\\n",
    "0.3600 - 0.2 (0.7200) = 0.2160 -- g: 2*2160 = 0.4320  \\\n",
    "0.2160 - 0.2 (0.4320) = 0.1296 -- g: 2*0.1296 = 0.2592 \\\n",
    "  ...  \\\n",
    "0.0060 - 0.2 (0.0121) = 0.00358\n",
    "\n",
    "> Finally, we solve the mystery of gradient >_< "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "73581038-fdb4-4d98-a3c3-5011b2c53f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "t = torch.Tensor([1])\n",
    "theta = torch.nn.Parameter(t)\n",
    "\n",
    "loss = theta * theta\n",
    "print(theta.grad)\n",
    "loss.backward()\n",
    "print(theta.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c465ea7-48b4-4b65-8d56-c7d522dec742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([1.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253b896c-0675-4faa-9eb9-1c30bb032437",
   "metadata": {},
   "source": [
    "The value of theta is not updated!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f75f2d3-9c0a-49f3-b693-d9c8c5001fec",
   "metadata": {},
   "source": [
    "#### Optimizer\n",
    "\n",
    "PyTorch optimizers automatically perform parameter updates based on the computed gradients.\n",
    "\n",
    "It has two methods: zero_grad() and step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5c435e70-5b85-46f2-ac8f-a53e3db85e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    differentiable: False\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.2\n",
       "    maximize: False\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_parameters = [theta]\n",
    "learning_rate = 0.2\n",
    "\n",
    "# each optimizer is initialised with a list of the parameters to optimize\n",
    "optimizer = torch.optim.SGD(params=list_parameters, lr=learning_rate)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b8d22a2f-cd8d-416f-915a-55102ed77ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1st loss.backward(): theta.grad=tensor([4.])\n",
      "After 2nd loss.backward(): theta.grad=tensor([6.])\n",
      "After 3rd loss.backward(): theta.grad=tensor([8.])\n"
     ]
    }
   ],
   "source": [
    "loss = theta * theta\n",
    "loss.backward()\n",
    "print(f\"After 1st loss.backward(): theta.grad={theta.grad}\")\n",
    "\n",
    "loss = theta * theta  # necessary to recompute and store\n",
    "loss.backward()\n",
    "print(f\"After 2nd loss.backward(): theta.grad={theta.grad}\")\n",
    "\n",
    "loss = theta * theta \n",
    "loss.backward()\n",
    "print(f\"After 3rd loss.backward(): theta.grad={theta.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b77dca-76f0-4909-8ed8-110b763d7d88",
   "metadata": {},
   "source": [
    "loss.backward() computes the gradients and adds those to the attributes .grad of the parameters. So to solve this zero_grad() is needed to reset gradients to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2a536f9a-3289-4faa-9516-aafe21b86ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1st loss.backward(): theta.grad=tensor([2.])\n",
      "After optimiser.zero_grad(): theta.grad=None\n",
      "After 2nd loss.backward(): theta.grad=tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "# zero_grad() : reset to 0\n",
    "optimiser = torch.optim.SGD(params=list_parameters, lr=learning_rate)\n",
    "optimiser.zero_grad()\n",
    "loss = theta * theta\n",
    "loss.backward()\n",
    "print(f\"After 1st loss.backward(): theta.grad={theta.grad}\")\n",
    "\n",
    "optimiser.zero_grad()\n",
    "print(f\"After optimiser.zero_grad(): theta.grad={theta.grad}\")\n",
    "\n",
    "loss = theta * theta   # necessary to recompute and store\n",
    "loss.backward()\n",
    "print(f\"After 2nd loss.backward(): theta.grad={theta.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f19d9357-d2e5-4b28-83ff-25e93f935a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before optimization step: Parameter containing:\n",
      "tensor([1.], requires_grad=True)\n",
      "After optimization step: Parameter containing:\n",
      "tensor([0.6000], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# step() : performs one optimisation step\n",
    "optimiser = torch.optim.SGD(params=list_parameters, lr=learning_rate)\n",
    "optimiser.zero_grad()\n",
    "loss = theta * theta\n",
    "loss.backward()\n",
    "print(f\"Before optimization step: {theta}\")\n",
    "optimiser.step()\n",
    "print(f\"After optimization step: {theta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e1b8a0cc-4737-4615-b685-6517a0aaae86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([ 0.6000, -0.6000], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.3600, -0.3600], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.2160, -0.2160], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1296, -0.1296], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0778, -0.0778], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0467, -0.0467], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0280, -0.0280], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0168, -0.0168], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0101, -0.0101], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0060, -0.0060], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.0060, -0.0060], requires_grad=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gradient descent with optimizer\n",
    "def gd_with_optimizer(initial_theta, lr, steps):\n",
    "    \"\"\" Returns gradient descents with the SGD optimiser. \"\"\"\n",
    "    tensor = torch.nn.Parameter(initial_theta, requires_grad=True)\n",
    "    optimizer = torch.optim.SGD(params=[tensor], lr=lr)\n",
    "    for _ in range(steps):\n",
    "        optimizer.zero_grad()\n",
    "        loss = torch.sum(tensor * tensor) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(tensor)\n",
    "    return tensor\n",
    "\n",
    "initial_tensor = torch.Tensor([1,-1]) \n",
    "gd_with_optimizer(initial_tensor, lr=0.2, steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5983b7eb-2242-4611-a5c4-377c39b00c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam_opt = torch.optim.Adam(params=list_parameters)\n",
    "adam_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "acf002a2-5bec-47b2-9125-0058dfd18265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RMSprop (\n",
       "Parameter Group 0\n",
       "    alpha: 0.99\n",
       "    capturable: False\n",
       "    centered: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    lr: 0.01\n",
       "    maximize: False\n",
       "    momentum: 0\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rms_opt = torch.optim.RMSprop(params=list_parameters)\n",
    "rms_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3fe585e3-af7d-401d-9fb6-170c8bcc0e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LBFGS (\n",
       "Parameter Group 0\n",
       "    history_size: 100\n",
       "    line_search_fn: None\n",
       "    lr: 1\n",
       "    max_eval: 25\n",
       "    max_iter: 20\n",
       "    tolerance_change: 1e-09\n",
       "    tolerance_grad: 1e-07\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbg_opt = torch.optim.LBFGS(params=list_parameters)\n",
    "lbg_opt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
