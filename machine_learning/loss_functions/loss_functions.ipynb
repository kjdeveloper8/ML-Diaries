{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58f512b6-404b-47c2-903d-be12bfb941da",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "A loss function is a mathematical function or expression used to measure how well a model is doing on dataset. Loss function takes a truth (y) and a prediction (Å·) as an input and produces a real valued score. The higher this score, the worse the model's prediction is.\n",
    "\n",
    "#### Loss function in pytorch\n",
    "\n",
    "All PyTorch's loss functions are packaged in the nn module, base class for all neural networks. This makes adding a loss function into model training easy.\n",
    "\n",
    "- Mean squared error (MSE)\n",
    "- L1 loss\n",
    "- Cross entropy\n",
    "- Binary cross entropy\n",
    "- Negative log likelihood\n",
    "- Smooth L1\n",
    "- Hinge embedding\n",
    "- Margin ranking\n",
    "- Triple margin\n",
    "- Cosine embedding\n",
    "- Custom loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1983ef-ca4d-4513-8686-0910dce6e103",
   "metadata": {},
   "source": [
    "### Mean Square Error (MES)\n",
    "\n",
    "MSE is similar to MAE. Instead of computing the absolute difference between values in the prediction tensor and target, as with MAE, it computes the square difference between values in the prediction tensor and that of the target tensor. By doing so, relatively large differences are penalized more, while relatively small differences are penalized less. MSE is considered less robust at handling outliers and noise than MAE though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08f87622-80b1-4926-89a6-0f79e616393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fae31bad-d82f-4bb4-80ab-fbcbed89405d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1142, -0.3803,  0.5391,  0.9389,  0.7259],\n",
       "        [ 1.5444, -0.3720, -1.3497, -0.1036,  0.7893],\n",
       "        [ 0.7241,  0.6515,  0.9762,  0.1345,  0.3507]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db1c9d00-85f2-4bf2-a0d7-09e1ef33c9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9189, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mse_loss = nn.MSELoss(size_average=None, reduce=None, reduction='mean')\n",
    "\n",
    "x = torch.randn(3, 5, requires_grad=True) # input\n",
    "target = torch.randn(3, 5)\n",
    "output = mse_loss(x, target)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6267ef76-7947-4949-aa5f-99158b45e708",
   "metadata": {},
   "source": [
    "### L1 loss function\n",
    "\n",
    "L1 loss function computes the mean absolute error (MAE) between each value in the predicted tensor and that of the target. It first calculates the absolute difference between each value in the predicted tensor and that of the target, and computes the sum of all the values returned from each absolute difference computation. Finally, it computes the average of this sum value to obtain the MAE. The L1 loss function is very robust for handling noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a3206d4-e01e-45f6-bc55-af19c47f856d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1165, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# reduction methods\n",
    "# mean: (default) compute the average of the output\n",
    "# sum: output is summed \n",
    "# none: no reduction to output\n",
    "\n",
    "l1_loss = nn.L1Loss(reduction='mean')\n",
    "\n",
    "x = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5)\n",
    "output = l1_loss(x, target)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae16210-dc48-44bc-8aa1-1f764ee095bf",
   "metadata": {},
   "source": [
    "### Cross entropy\n",
    "\n",
    "Cross-entropy loss is used in classification problems involving a number of discrete classes. It measures the difference between two probability distributions for a given set of random variables. Usually, when using cross-entropy, the output of our network is a softmax layer, which ensures that the output of the neural network is a probability value between 0 - 1.\n",
    "\n",
    "Cross-entropy and the expression for it have origins in information theory. We want the probability of the correct class to be close to 1, whereas the other classes have a probability close to 0.\n",
    "\n",
    "For Pytorch's CrossEntropyLoss function, we need to determine the relationship between network output and loss function. \n",
    "1. There is a limit to how small or how large a number can be.\n",
    "2. If input to the exponential function used in the softmax formula is a negative number, the resultant is an exponentially small number, and if it's a positive number, the resultant is an exponentially large number.\n",
    "3. The network's output is assumed to be the vector just prior to applying the softmax function.\n",
    "4. The log function is the inverse of the exponential function, and log(exp(x)) is just equal to x.\n",
    "\n",
    "Mathematical simplifications are made assuming the exponential function that is the core of the softmax function and the log function that is used in the cross-entropy computations in order to be more numerically stable and avoid really small or really large numbers. The consequences of these simplifications are that the network output without the use of a softmax function can be used in conjunction with PyTorch's CrossEntropyLoss() to optimize the probability distribution. Then, when the network has been trained, the softmax function can be used to create a probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e92357f7-b6a9-4d65-9b05-a4d89276208c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0496, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# CrossEntropyLoss() assumes that each input has one particular class\n",
    "# and each class has a unique index\n",
    "# therefore ground truth vector (targets) is created as a vector of integers\n",
    "# and has index representing the correct class for each input\n",
    "\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "outputs = torch.randn(3, 5, requires_grad=True) \n",
    "targets = torch.tensor([1, 0, 3], dtype=torch.int64) \n",
    "loss = ce_loss(outputs, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d40224f-e306-4521-a9f0-c0a95125f956",
   "metadata": {},
   "source": [
    "### Binary Corss entropy\n",
    "\n",
    "Binary cross-entropy loss is a special class of cross-entropy losses used for binary classification. Usually when using BCE loss for binary classification, the output of the neural network is a Sigmoid layer to ensure that the output is either a value close to zero or a value close to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf07542c-f4af-4211-8d7b-c96669db4021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1922],\n",
      "        [0.5377],\n",
      "        [0.6821],\n",
      "        [0.6447]], grad_fn=<SigmoidBackward0>)\n",
      "loss=tensor(2.0496, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "bce_loss = nn.BCELoss()\n",
    "sigmoid = nn.Sigmoid()\n",
    "probabilities = sigmoid(torch.randn(4, 1, requires_grad=True)) \n",
    "targets = torch.tensor([1, 0, 1, 0], dtype=torch.float32).view(4, 1)\n",
    "oss = bce_loss(probabilities, targets) \n",
    "print(probabilities)\n",
    "print(f'{loss=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e4b827-07d9-40e0-98f9-7cfd16215b3f",
   "metadata": {},
   "source": [
    "### Negative Log likelihood\n",
    "\n",
    "The NLL loss function works quite similarly to the cross-entropy loss function. As cross-entropy loss combines a log-softmax layer and NLL loss. This means that NLL loss can be used to obtain the cross-entropy loss value by having the last layer of the neural network be a log-softmax layer instead of a normal softmax layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d429699-2a03-4e34-852f-a3700e383f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0164, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "m = nn.LogSoftmax(dim=1)\n",
    "loss = nn.NLLLoss()\n",
    "\n",
    "x = torch.randn(3, 5, requires_grad=True)           # input size N x C = 3 x 5\n",
    "target = torch.tensor([1, 0, 4])                    # each element have 0 <= value < C\n",
    "output = loss(m(x), target)\n",
    "output.backward()\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1654d7-3f17-41fc-9bcd-0c01b8d6a8f1",
   "metadata": {},
   "source": [
    "### Smooth L1 loss\n",
    "\n",
    "The smooth L1 loss function combines the benefits of MSE loss and MAE loss through a heuristic value beta. When the absolute difference between the ground truth value and the predicted value is below beta, the criterion uses a squared difference, much like MSE loss(gradient at each loss value varies and can be derived everywhere). Moreover, as the loss value reduces the gradient diminishes, which is convenient during gradient descent. However for very large loss values the gradient explodes, hence the criterion switching to a MAE, whose gradient is almost constant for every loss value, when the absolute difference becomes larger than beta and the potential gradient explosion is eliminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d052d2a-0bec-4499-bc8c-6cd0d29ae1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7836, grad_fn=<SmoothL1LossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sl1_loss = nn.SmoothL1Loss()\n",
    "x = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5)\n",
    "output = sl1_loss(x, target)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583a6816-6f88-47b1-a009-af778e36ec2d",
   "metadata": {},
   "source": [
    "### Hinge embedding loss\n",
    "\n",
    "Hinge embedding loss is mostly used in semi-supervised learning tasks to measure the similarity between two inputs. It's used when there is an input tensor and a label tensor containing values of 1 or -1. It is mostly used in problems involving non-linear embeddings and semi-supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e1abdb2-7a0a-4523-b08f-ab46a2840900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=tensor([[-0.0382, -1.1631, -3.0978,  1.3347,  0.9333],\n",
      "        [-1.1167,  0.1461, -0.4923, -0.3534,  0.5138],\n",
      "        [ 1.3475,  0.2561, -2.8684,  1.5239, -0.0787]], requires_grad=True)\n",
      "target=tensor([[-0.7516, -1.3190,  0.0808, -0.5089, -2.0764],\n",
      "        [-0.6985,  0.1275,  0.5060, -1.3018,  1.2995],\n",
      "        [-0.4525,  0.3468, -0.5632,  1.7237, -1.7925]])\n",
      "output=tensor(1.0804, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5)\n",
    "\n",
    "hinge_loss = nn.HingeEmbeddingLoss()\n",
    "output = hinge_loss(x, target)\n",
    "output.backward()\n",
    "\n",
    "print(f'{x=}')\n",
    "print(f'{target=}')\n",
    "print(f'{output=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b47570-0c45-4d46-a704-e52a273a0e3f",
   "metadata": {},
   "source": [
    "### Margin ranking loss\n",
    "\n",
    "Margin ranking loss belongs to the ranking losses which is used to measure the relative distance between a set of inputs in a dataset. It takes two inputs and a label containing only 1 or -1. \n",
    "- If the label is 1, then it is assumed that the first input should have a higher ranking than the second input.\n",
    "- If the label is -1, then it is assumed that the second input should have a higher ranking than the first input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8cc078c-e88c-4c63-882b-068cea7d533e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input1=tensor([-0.2377, -0.7254, -2.4694], requires_grad=True)\n",
      "input2=tensor([-0.0987, -1.4337,  1.0014], requires_grad=True)\n",
      "output=tensor(0.2824, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mr_loss = nn.MarginRankingLoss()\n",
    "input1 = torch.randn(3, requires_grad=True)\n",
    "input2 = torch.randn(3, requires_grad=True)\n",
    "target = torch.randn(3).sign()\n",
    "output = mr_loss(input1, input2, target)\n",
    "print(f'{input1=}')\n",
    "print(f'{input2=}')\n",
    "print(f'{output=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e281755-16eb-4d7e-aedc-504726a74a4f",
   "metadata": {},
   "source": [
    "### Triple margin loss\n",
    "\n",
    "This measures data points by using triplets of the training data sample. The triplets involved are an anchor sample, a positive sample and a negative sample. \n",
    "1. to get the distance between the positive sample and the anchor as minimal as possible\n",
    "2. to get the distance between the anchor and the negative sample to have greater than a margin value plus the distance between the positive sample and the anchor.\n",
    "  \n",
    "Usually, the positive sample belongs to the same class as the anchor, but the negative sample does not. We use triplet margin loss to predict a high similarity value between the anchor and the positive sample and a low similarity value between the anchor and the negative sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf1a08d3-505b-422e-a6e2-2423214c07b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1436, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "anchor = torch.randn(100, 128, requires_grad=True)\n",
    "positive = torch.randn(100, 128, requires_grad=True)\n",
    "negative = torch.randn(100, 128, requires_grad=True)\n",
    "output = triplet_loss(anchor, positive, negative)\n",
    "print(output) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827bfce8-d874-4f51-9ae2-065ff1e4edba",
   "metadata": {},
   "source": [
    "### Cosine embedding loss\n",
    "\n",
    "Cosine embedding loss measures the loss given inputs x1, x2, and a label tensor y containing values 1 or -1. It is used for measuring the degree to which two inputs are similar or dissimilar by computing the cosine distance between the two data points in space. The cosine distance correlates to the angle between the two points which means that the smaller the angle, the closer the inputs and hence the more similar they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "984801db-dcb5-4991-a5f0-663299f3ba11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input1=tensor([[ 0.3354, -0.7056, -0.0219, -0.0693, -0.8870, -1.7239],\n",
      "        [-0.3579,  0.7590,  0.5300,  0.2401, -0.9892,  0.4002],\n",
      "        [ 1.3142, -0.3310, -0.1450, -0.5311,  0.0077,  0.2826]],\n",
      "       requires_grad=True)\n",
      "input2=tensor([[ 0.0281, -0.3903,  0.7034,  0.3182, -0.3571,  0.1982],\n",
      "        [-0.4479,  0.1134,  0.9433, -0.0840, -0.0399, -2.2796],\n",
      "        [ 0.3697,  0.0200, -1.2353,  0.2872,  0.6938,  0.2455]],\n",
      "       requires_grad=True)\n",
      "output=tensor(0.7287, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cosine_loss = nn.CosineEmbeddingLoss()\n",
    "input1 = torch.randn(3, 6, requires_grad=True)\n",
    "input2 = torch.randn(3, 6, requires_grad=True)\n",
    "target = torch.randn(3).sign()\n",
    "output = cosine_loss(input1, input2, target)\n",
    "print(f'{input1=}')\n",
    "print(f'{input2=}')\n",
    "print(f'{output=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2e12d0-9ec2-448c-abeb-3ee9ccaeb4af",
   "metadata": {},
   "source": [
    "### Custom function\n",
    "\n",
    "PyTorch provides ways to build our own loss function - A custom loss function to calculate the mean square error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13f2738c-698d-4e79-a9df-e59fcdf0a80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_mean_square_error(y_pred, target):\n",
    "  square_difference = torch.square(y_pred - target)\n",
    "  loss_value = torch.mean(square_difference)\n",
    "  return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c10f232-6243-4cf8-8764-9baadde44d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_loss=tensor(1.2872, grad_fn=<MeanBackward0>)\n",
      "p_loss=tensor(1.2872, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y_pred = torch.randn(3, 5, requires_grad=True);\n",
    "target = torch.randn(3, 5)\n",
    "pytorch_loss = nn.MSELoss();\n",
    "p_loss = pytorch_loss(y_pred, target)\n",
    "custom_loss = custom_mean_square_error(y_pred, target)\n",
    "print(f'{custom_loss=}')\n",
    "print(f'{p_loss=}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
